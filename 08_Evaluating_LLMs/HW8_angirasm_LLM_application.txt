A potential use case for LLMs for science that my research group has been interested in is the learning of SMILES (Simplified molecular-input line-entry system) strings.
SMILES strings enable the description of the structure of chemical species using short ASCII strings and so have a simple and learnable grammar that an LLM could use an inputs. 
By training on the large amount of chemical space, an LLM could in principle learn to correlate a SMILES string to whatever classification or property prediction task of interest to the user. 

Other architectures already do this like chemprop (https://github.com/chemprop/chemprop/) which creates D-MPNNs from SMILES strings that can then be trained to develop a learned representation and passed to an FFN for readout. 

In prinicple training and evaluation of an LLM would work in much the same way as for other architectures. An example already comes from Meta's Galactica model who trained on scientific literature and peer-reviewed publications
to develop a classifier for properties such as blood-brain barrier penetration and protein affinity/activity. The evaluation would make use of standard accuracy metrics like accuray and F1 score. 

For our particular group, we would be more interested in the generative side of applications. Particularly, by studying several reaction SMILES (where reaction transformations are encoded using SMILES for reactants and products) 
and understanding how energetically favorable different reactions are (based on their free energy of activation, for example) an LLM could learn how transformations on SMILES strings relate to viable reactive chemistry. 
Then the model could be used as part of the reaction mechanism generation pipeline, proposing new transformations on the basis of an input reaction SMILES and doing so in a sequential manner until a certain size of reaction 
mechanism is reached. To evaluate such a model, we would first check the viability of the reactions generated. This would involve computing the true activation free energies of the reaction and scoring based on how many 
feasible reactions the LLM suggests. Ideally, the LLM would be pre-trained to predict barrier heights first, and the generative aspect would use Bayesian approaches to optimize for reactions that are most viable. 
Another evaluation would be on completeness when benchmarking. For example, there are several highly studied and well-established combustion mechanisms in the literature that have a refined set of reactions. 
The mechanism the LLM generates would be benchmarked against these established mechanisms first to see if it missed any reactions that are established to be important before being applied to generate mechanisms for new systems. 

One other application would be a text to image idea that has been discussed by several research groups: going from a simple string representation of a molecule like SMILES to an actual image/structure of a molecule. This can 
be evaluated like a classifier with how accurate the LLM generated structure is based on the SMILES prompted. A further advancement of this idea that I would like to see is applying LLMs to not just generate a single structure from a SMILES string
but to be a conformer generator like ETKDG within RDKit for example. This would be evaluated by taking the set of conformers generated by the LLM and computing their electronic energies using standard computational chemical methods. The conformer 
set generated by the LLM could then be compared to those generated by established conformer generation methods. Deep learning models based on graphs and transformers already exist that do this (GeoMol and UniMol) but the vast size and diversity 
of pre-training available to LLMs are currently unmatched. Seeing if this translates into generative capability is something that I would like to test in the future. 