{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/am2145/ai-science-training-series/blob/homeworks/02_intro_neural_networks/HW2_angirasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhgWSjOUtZ1T"
      },
      "source": [
        "# Introduction to Neural Networks\n",
        "Author: Bethany Lusch, adapting materials from Marieme Ngom, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, and Tanwi Mallick.\n",
        "\n",
        "This tutorial will serve as a gentle introduction to neural networks and deep learning through a hands-on classification problem using the MNIST dataset.\n",
        "\n",
        "In particular, we will introduce neural networks and how to train and improve their learning capabilities.  We will use the PyTorch Python library.\n",
        "\n",
        "The [MNIST dataset](http://yann.lecun.com/exdb/mnist/) contains thousands of examples of handwritten numbers, with each digit labeled 0-9.\n",
        "<img src=\"https://github.com/am2145/ai-science-training-series/blob/homeworks/02_intro_neural_networks/images/mnist_task.png?raw=1\"  align=\"left\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIcLCu0QtZ1U"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GogbSXcVtZ1U"
      },
      "source": [
        "## The MNIST dataset\n",
        "\n",
        "We will now download the dataset that contains handwritten digits. MNIST is a popular dataset, so we can download it via the PyTorch library. Note:\n",
        "- x is for the inputs (images of handwritten digits) and y is for the labels or outputs (digits 0-9)\n",
        "- We are given \"training\" and \"test\" datasets. Training datasets are used to fit the model. Test datasets are saved until the end, when we are satisfied with our model, to estimate how well our model generalizes to new data.\n",
        "\n",
        "Note that downloading it the first time might take some time.\n",
        "The data is split as follows:\n",
        "- 60,000 training examples, 10,000 test examples\n",
        "- inputs: 1 x 28 x 28 pixels\n",
        "- outputs (labels): one integer per example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VnED0zrtZ1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca00e1c8-0e1f-414f-e021-0893dbcf9123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 83623224.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 48825350.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 24325605.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 10286462.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "training_data = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_data = torchvision.datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPnA0Q5_tZ1V"
      },
      "outputs": [],
      "source": [
        "training_data, validation_data = torch.utils.data.random_split(training_data, [0.8, 0.2], generator=torch.Generator().manual_seed(55))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWJEZPKKtZ1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe5fce7-6d1d-41c4-a6d7-73a5b5ef4ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST data loaded: train: 48000  examples, validation:  12000 examples, test: 10000 examples\n",
            "Input shape torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print('MNIST data loaded: train:',len(training_data),' examples, validation: ', len(validation_data), 'examples, test:',len(test_data), 'examples')\n",
        "print('Input shape', training_data[0][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HPQHDsmtZ1V"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# The dataloader makes our dataset iterable\n",
        "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
        "val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bpnEp9EtZ1W"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # forward pass\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # backward pass calculates gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # take one step with these gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # resets the gradients\n",
        "        optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obeiz9-ItZ1W"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    loss, correct = 0, 0\n",
        "\n",
        "    # We can save computation and memory by not calculating gradients here - we aren't optimizing\n",
        "    with torch.no_grad():\n",
        "        # loop over all of the batches\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            loss += loss_fn(pred, y).item()\n",
        "            # how many are correct in this batch? Tracking for accuracy\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    accuracy = 100*correct\n",
        "    return accuracy, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhfuSGiotZ1W"
      },
      "outputs": [],
      "source": [
        "def show_failures(model, dataloader, maxtoshow=10):\n",
        "    model.eval()\n",
        "    batch = next(iter(dataloader))\n",
        "    predictions = model(batch[0])\n",
        "\n",
        "    rounded = predictions.argmax(1)\n",
        "    errors = rounded!=batch[1]\n",
        "    print('Showing max', maxtoshow, 'first failures. '\n",
        "          'The predicted class is shown first and the correct class in parentheses.')\n",
        "    ii = 0\n",
        "    plt.figure(figsize=(maxtoshow, 1))\n",
        "    for i in range(batch[0].shape[0]):\n",
        "        if ii>=maxtoshow:\n",
        "            break\n",
        "        if errors[i]:\n",
        "            plt.subplot(1, maxtoshow, ii+1)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(batch[0][i,0,:,:], cmap=\"gray\")\n",
        "            plt.title(\"%d (%d)\" % (rounded[i], batch[1][i]))\n",
        "            ii = ii + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38kTsJVrtZ1W"
      },
      "source": [
        "<!-- # Exercise:\n",
        "- Try changing the loss function,\n",
        "- Try changing the optimizer -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvIlM9X7tZ1X"
      },
      "outputs": [],
      "source": [
        "class NonlinearClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layers_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(50, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.layers_stack(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nOyqreotZ1X"
      },
      "source": [
        "# Homework: train a Nonlinear Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KBVrGU3tZ1X"
      },
      "source": [
        "1. Write some code to train the NonlinearClassifier.\n",
        "2. Create a data loader for the test data and check your model's accuracy on the test data.\n",
        "\n",
        "If you have time, experiment with how to improve the model. Note: training and validation data can be used to compare models, but test data should be saved until the end as a final check of generalization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader(dataset, batch_size):\n",
        "  return torch.utils.data.DataLoader(dataset, batch_size = batch_size)\n"
      ],
      "metadata": {
        "id": "vmaFGvckt56P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_n_epochs(train_dataloader, val_dataloader, model, loss_fn, num_epochs, lr, verbose = True):\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "  for n in range(num_epochs):\n",
        "    train_one_epoch(train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "    train_acc, train_loss = evaluate(train_dataloader, model, loss_fn)\n",
        "    val_acc, val_loss = evaluate(val_dataloader, model, loss_fn)\n",
        "\n",
        "    if verbose:\n",
        "      # checking on the training loss and accuracy once per epoch\n",
        "      print(f\"Epoch {n}: training loss: {train_loss}, accuracy: {train_acc}\")\n",
        "\n",
        "      # checking on the validation loss and accuracy once per epoch\n",
        "      print(f\"Epoch {n}: validation loss: {val_loss}, accuracy: {val_acc}\")"
      ],
      "metadata": {
        "id": "WFAkzdJ8t50X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "my_batch_size = 128\n",
        "train_dataloader = dataloader(training_data, my_batch_size)\n",
        "val_dataloader = dataloader(validation_data, my_batch_size)"
      ],
      "metadata": {
        "id": "T1CF4gDEt5l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize model and loss function\n",
        "nonlinear_model = NonlinearClassifier()\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "zjiTvqzvvZjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Select number of epochs, learning rate, and optimizer.\n",
        "n_epochs = 5\n",
        "lr = 0.1\n"
      ],
      "metadata": {
        "id": "zoz0BcBgvZg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_n_epochs(train_dataloader, val_dataloader, nonlinear_model, loss_fn, n_epochs, lr) #try an example."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAUzX5dBvZen",
        "outputId": "e3bd8cf7-782d-4995-9823-9a6f3b242228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: training loss: 0.3620865137974421, accuracy: 89.63958333333333\n",
            "Epoch 0: validation loss: 0.3583734010128265, accuracy: 89.55\n",
            "Epoch 1: training loss: 0.2535987759629885, accuracy: 92.58958333333334\n",
            "Epoch 1: validation loss: 0.2500910741534639, accuracy: 92.45833333333333\n",
            "Epoch 2: training loss: 0.21031810522079467, accuracy: 93.72708333333334\n",
            "Epoch 2: validation loss: 0.21044380891513317, accuracy: 93.63333333333334\n",
            "Epoch 3: training loss: 0.1797093785703182, accuracy: 94.66041666666666\n",
            "Epoch 3: validation loss: 0.18371592065755357, accuracy: 94.27499999999999\n",
            "Epoch 4: training loss: 0.16030347616473833, accuracy: 95.1875\n",
            "Epoch 4: validation loss: 0.16458108561470153, accuracy: 95.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the test data and evaluate on it.\n",
        "test_dataloader = dataloader(test_data, my_batch_size)\n",
        "test_acc, test_loss = evaluate(test_dataloader, nonlinear_model, loss_fn)\n",
        "print(f\"Final model testing loss: {test_loss}, accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzgcRxzSwL52",
        "outputId": "038621ad-3a7e-4ec6-a986-bf04f274388f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model testing loss: 0.1644941335514518, accuracy: 94.91000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools as it\n",
        "\n",
        "def get_dict_combinations(input_dict):\n",
        "  allNames = sorted(input_dict)\n",
        "  combinations = it.product(*(input_dict[Name] for Name in allNames))\n",
        "  comb_list = list(combinations)\n",
        "  return comb_list"
      ],
      "metadata": {
        "id": "cF2gI3pby9FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Some basic hyperparameter searching:\n",
        "\n",
        "def tune_train_nonlinear(param_dict):\n",
        "  test_accs = []\n",
        "  test_losses = []\n",
        "  param_combs = get_dict_combinations(param_dict)\n",
        "\n",
        "  for combination in param_combs:\n",
        "    n_epochs = combination[1]\n",
        "    lr = combination[0]\n",
        "\n",
        "    nonlinear_model = NonlinearClassifier()\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_val_n_epochs(train_dataloader, val_dataloader, nonlinear_model, loss_fn, n_epochs, lr, verbose=False)\n",
        "    test_acc, test_loss = evaluate(test_dataloader, nonlinear_model, loss_fn)\n",
        "    print(f\"Final model testing loss: {test_loss}, accuracy: {test_acc}\")\n",
        "    test_accs.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "  return test_accs, test_losses\n",
        "\n"
      ],
      "metadata": {
        "id": "Pf15GLVnwL33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict = {'n_epochs': [5,10], 'lr' : [0.05, 0.1]}"
      ],
      "metadata": {
        "id": "ts0S5PMg0GG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_combs = get_dict_combinations(param_dict)\n",
        "param_combs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paLfGfbv31e2",
        "outputId": "7e2879de-db95-4ccb-862f-ba05442f4f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.05, 5), (0.05, 10), (0.1, 5), (0.1, 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tune_train_nonlinear(param_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh-QJ43g1fAn",
        "outputId": "945d65d1-de18-4d71-b826-cfacf41703b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model testing loss: 0.22767401788430877, accuracy: 93.03\n",
            "Final model testing loss: 0.15926099219064735, accuracy: 94.99\n",
            "Final model testing loss: 0.1722397496776443, accuracy: 94.75\n",
            "Final model testing loss: 0.11935077312319906, accuracy: 96.46000000000001\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([93.03, 94.99, 94.75, 96.46000000000001],\n",
              " [0.22767401788430877,\n",
              "  0.15926099219064735,\n",
              "  0.1722397496776443,\n",
              "  0.11935077312319906])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsBPOl26tZ1X"
      },
      "source": [
        "# JupyterHub Reminder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH6yPPMqtZ1X"
      },
      "source": [
        "From [Homework 0](https://github.com/argonne-lcf/ai-science-training-series/blob/main/00_introToAlcf/02_jupyterNotebooks.md): \"If you simply close your browser window, or logout without shutting down the jupyter server, your job will continue to occupy the worker node. Be considerate and shutdown your job when you finish.\"\n",
        "\n",
        "File --> Hub Control Panel --> Stop my server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4l5m-7FtZ1X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "common_pkgs",
      "language": "python",
      "name": "common_pkgs"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}